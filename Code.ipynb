{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJxi3L5w1VR",
        "outputId": "e297a04a-bc80-43d2-d918-eca106e5e264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW"
      ],
      "metadata": {
        "id": "Ll_nwmV6w6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOYDdrMzw6xK",
        "outputId": "4920a190-f33b-4380-b84e-760828041b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "def dataloader(dataset) :\n",
        "    import pandas as pd\n",
        "    import re  # Import the 're' module for regular expressions\n",
        "\n",
        "    # Define the path to your CSV file\n",
        "    csv_file_path = '/content/anomaly_label.csv'\n",
        "\n",
        "    # Read the CSV file using pandas\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Define the function to generate labels based on \"alias_list\"\n",
        "    def generate_labels(alias_list):\n",
        "        search_pattern = r\"Unknown host\"\n",
        "        if pd.notna(alias_list) and re.search(search_pattern, alias_list):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # Apply the function to create the \"labels\" column\n",
        "    df[\"labels\"] = df[\"alias_list\"].apply(generate_labels)\n",
        "\n",
        "    # Save the modified DataFrame back to the CSV file\n",
        "    df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "     # Specify the column you want to read\n",
        "    target_column_name_1 = 'client'  # Replace with the name of the desired column\n",
        "    target_column_name_2 = 'labels'\n",
        "\n",
        "\n",
        "\n",
        "    labels=[]\n",
        "    n= len(df)-2\n",
        "    print(n)\n",
        "\n",
        "\n",
        "    with open(dataset, 'r') as file:\n",
        "        log_entries = file.readlines()\n",
        "\n",
        "    for i in range(1,n):\n",
        "        column_values_1 = df.loc[i,target_column_name_1]\n",
        "        column_values_2 = df.loc[i,target_column_name_2]\n",
        "        for log_entry in log_entries:\n",
        "            # Define a regex pattern to match IP addresses\n",
        "            ip_address_pattern = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
        "\n",
        "            # Find the IP address using regex\n",
        "            ip_address_match=re.search(ip_address_pattern, log_entry)\n",
        "            ip_address = ip_address_match.group()\n",
        "\n",
        "            if((column_values_1==ip_address) and (column_values_2==0)):\n",
        "                labels.append(0)\n",
        "\n",
        "            elif ((column_values_1==ip_address) and (column_values_2==1)):\n",
        "                labels.append(1)\n",
        "\n",
        "    #print(labels)\n",
        "\n",
        "    max_sequence_length = 128\n",
        "\n",
        "\n",
        "    texts = log_entries\n",
        "\n",
        "    lol=[]\n",
        "    lol.append(labels)\n",
        "\n",
        "    # Tokenize and preprocess the dataset\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_text = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=max_sequence_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    input_ids.append(encoded_text['input_ids'])\n",
        "    print(input_ids)\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "    attention_masks.append(encoded_text['attention_mask'])\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(lol)\n",
        "\n",
        "    return  input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "jCmrbXAzw7SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=dataloader(dataset=\"/content/train.log\")\n",
        "input_ids, attention_masks, labels=x\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB25MPtLw7Vf",
        "outputId": "1cde3e42-c7f9-4703-c291-39a9eeaf1087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258443\n",
            "[tensor([[  101,  7558,  1012, 15376,  1012,  6356,  1012, 22884,  1011,  1011,\n",
            "          1031,  2570,  1013,  5553,  1013, 10476,  1024,  5840,  1024,  6021,\n",
            "          1024,  4090,  1009,  6021, 14142,  1033,  1000,  2131,  1013, 11307,\n",
            "          1013, 26261,  9048, 12837,  1010,  1052, 18827,  2692,  2620,  2475,\n",
            "          8299,  1013,  1015,  1012,  1015,  1000,  3263, 21036, 12521,  1000,\n",
            "          1011,  1000,  1000,  9587,  5831,  4571,  1013,  1019,  1012,  1014,\n",
            "          1006,  3645, 23961,  1020,  1012,  1015,  1025, 27634,  1024,  4413,\n",
            "          1012,  1014,  1007, 16216, 19665,  1013,  2230, 24096, 24096,  2543,\n",
            "         14876,  2595,  1013,  4413,  1012,  1014,  1000,  1000,  1011,  1000,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compliance labels\n",
        "compliance_labels = {\n",
        "    0: 'Non-Compliant',\n",
        "    1: 'Compliant'\n",
        "}"
      ],
      "metadata": {
        "id": "PcCNak1KxL-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define your optimizer\n",
        "learning_rate = 0.001\n",
        "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# Create an instance of your DataLoader using your preprocessed dataset\n",
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, attention_mask, labels = batch\n",
        "        outputs = model(inputs, attention_mask)\n",
        "        #loss = loss_fn(outputs, labels)  # You need to define your loss function\n",
        "        #loss_fn.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYo3MUk9xMB3",
        "outputId": "afdc017d-eb1a-4d81-9198-d5de365a4c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the test log file\n",
        "with open('test.log', 'r') as file:\n",
        "    log_entries = file.readlines()\n",
        "pred_label=[]\n",
        "# Process each log entry and generate compliance predictions\n",
        "for log_entry in log_entries:\n",
        "    inputs = tokenizer(log_entry, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_label = torch.argmax(outputs.logits).item()\n",
        "    pred_label.append(predicted_label)\n",
        "    compliance_label = list(compliance_labels.keys())[predicted_label]\n",
        "    predicted_compliance = compliance_labels[predicted_label]"
      ],
      "metadata": {
        "id": "p24BYUBqxX0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "\n",
        "# Create a tensor\n",
        "#tensor = torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "# Convert tensor to a list\n",
        "tensor_list = labels.tolist()\n",
        "\n",
        "#print(tensor_list)  # Output: [1, 2, 3, 4, 5]\n",
        "\n",
        "ground_truth_labels=[]\n",
        "ground_truth_labels=tensor_list[0]\n",
        "\n",
        "# Assuming you have lists of predicted_labels and true_labels\n",
        "# Replace with actual predicted labels (0 or 1)\n",
        "true_labels = ground_truth_labels  # Replace with actual true labels (0 or 1)\n",
        "\n",
        "# Initialize counts for TP, TN, FP, and FN\n",
        "tp = tn = fp = fn = 0\n",
        "\n",
        "# Calculate TP, TN, FP, FN\n",
        "for pred, true in zip(pred_label, true_labels):\n",
        "    if pred == 1 and true == 1:\n",
        "        tp += 1\n",
        "    elif pred == 0 and true == 0:\n",
        "        tn += 1\n",
        "    elif pred == 1 and true == 0:\n",
        "        fp += 1\n",
        "    elif pred == 0 and true == 1:\n",
        "        fn += 1\n",
        "\n",
        "# Calculate accuracy, precision, and recall\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0o_w2EoxX4P",
        "outputId": "c0331c82-44a6-4ad1-b2f7-94d0224004f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives: 27\n",
            "True Negatives: 5\n",
            "False Positives: 86\n",
            "False Negatives: 3\n",
            "Accuracy: 0.26\n",
            "Precision: 0.24\n",
            "Recall: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the vad log file\n",
        "with open('vad.log', 'r') as file:\n",
        "    log_entries = file.readlines()\n",
        "pred_label=[]\n",
        "# Process each log entry and generate compliance predictions\n",
        "for log_entry in log_entries:\n",
        "    inputs = tokenizer(log_entry, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_label = torch.argmax(outputs.logits).item()\n",
        "    pred_label.append(predicted_label)\n",
        "    compliance_label = list(compliance_labels.keys())[predicted_label]\n",
        "    predicted_compliance = compliance_labels[predicted_label]\n",
        "    print(f\"Log Entry: {log_entry}\")\n",
        "    print(f\"Predicted Compliance: {compliance_label}\\n\")\n",
        "    print(f\"Predicted Compliance: {predicted_compliance}\")\n",
        "    print(pred_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdVxRB4Cxgxm",
        "outputId": "8deece8c-7640-48c8-8aab-51024f4f5108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Entry: 31.56.102.211 - - [22/Jan/2019:17:42:12 +0330] \"GET /image/34290?name=galaxy-j2-core-1.jpg&wh=200x200 HTTP/1.1\" 200 4328 \"https://torob.com/search/?query=%D8%A7%D9%84%20%D8%B3%DB%8C%20%D8%AF%DB%8C%20%DA%AF%D9%84%DA%A9%D8%B3%DB%8C%20j2\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\" \"-\"\n",
            "\n",
            "Predicted Compliance: 0\n",
            "\n",
            "Predicted Compliance: Non-Compliant\n",
            "[0]\n",
            "Log Entry: 5.123.144.95 - - [24/Jan/2019:23:53:52 +0330] \"GET /image/33888?name=model-b2048u-1-.jpg&wh=200x200 HTTP/1.1\" 200 3975 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.0.0; ALP-L29 Build/HUAWEIALP-L29S)\" \"-\"\n",
            "\n",
            "Predicted Compliance: 1\n",
            "\n",
            "Predicted Compliance: Compliant\n",
            "[0, 1]\n"
          ]
        }
      ]
    }
  ]
}